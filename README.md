# IaC-Bench

> Benchmarking LLMs and Agents on Real-World Infrastructure Engineering Tasks

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-early%20development-yellow)](https://github.com/yourusername/iac-bench)

IaC-Bench is an open-source benchmark suite for evaluating LLMs and autonomous agents on real-world Infrastructure as Code tasks. Inspired by [SWE-bench](https://www.swebench.com/) and [IaC-Eval](https://proceedings.neurips.cc/paper_files/paper/2024/file/f26b29298ae8acd94bd7e839688e329b-Paper-Datasets_and_Benchmarks_Track.pdf), we focus on IaC code generation, modification, and validation.

## ğŸ¯ Goals

- **Real-world IaC tasks** from actual infrastructure projects
- **Comprehensive evaluation** of LLMs and agents on IaC code generation and modification
- **Reproducible testing** with containerized environments
- **Continuous evolution** with community-driven task curation

## âœ¨ Features

- ğŸ“¦ Real IaC tasks (provisioning, configuration, scaling, troubleshooting)
- â˜ï¸ Multi-cloud support (AWS, Azure, GCP, Kubernetes)
- ğŸ”§ Multiple IaC tools (Terraform, Ansible, Pulumi, CloudFormation)
- ğŸ§ª Automated evaluation framework with Dockerized test environments
- ğŸ“Š Standardized metrics for correctness, best practices, and security compliance

## ğŸš€ Quick Start

_Coming soon - installation and usage instructions_

## ğŸ“‹ Task Categories

- **Provisioning**: Cloud resource creation and configuration
- **Configuration**: Infrastructure configuration management
- **Scaling**: Resource optimization and scaling strategies
- **Security**: Compliance and security configurations
- **Troubleshooting**: IaC code issue resolution

## ğŸ—ï¸ Approach

Tasks are curated from:
- Open-source infrastructure repositories
- Real GitHub issues and pull requests
- Industry use cases and best practices
- Community contributions

Each task includes problem statements, context, expected outcomes, and automated evaluation criteria.

## ğŸ¤ Contributing

Contributions welcome! We're looking for help with:
- Task curation and validation
- Evaluation framework development
- Test environment setup
- Documentation improvements

## ğŸ’° Sponsors

<a href="https://cloudgeni.com"><img src="https://cloudgeni.com/logo.png" alt="Cloudgeni" width="150"></a>

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details

## ğŸ™ Acknowledgments

Inspired by [SWE-bench](https://www.swebench.com/) and [IaC-Eval](https://proceedings.neurips.cc/paper_files/paper/2024/file/f26b29298ae8acd94bd7e839688e329b-Paper-Datasets_and_Benchmarks_Track.pdf).

---

**Note**: This project is in early development. Structure and approach may evolve based on community feedback.
